{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg"
      ],
      "metadata": {
        "id": "l8WK6RCbzmab"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_image_path = 'http://guereak.com/nst/base_image_1.jpg'\n",
        "style_reference_image_path_1 = 'http://guereak.com/nst/style_transfer_1.jpg'\n",
        "style_reference_image_path_2 = 'http://guereak.com/nst/style_transfer_2.jpg'\n",
        "style_reference_image_path_3 = 'http://guereak.com/nst/style_transfer_3.jpg'\n",
        "style_reference_image_path_4 = 'http://guereak.com/nst/style_transfer_4.jpg'\n",
        "\n",
        "style_reference_image_path = style_reference_image_path_3"
      ],
      "metadata": {
        "id": "h7JwOIWZxOlH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Download and load the images"
      ],
      "metadata": {
        "id": "P8HggzWjckMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_image_path = keras.utils.get_file(origin=base_image_path)\n",
        "style_reference_image_path = keras.utils.get_file(origin=style_reference_image_path)"
      ],
      "metadata": {
        "id": "qbI8YW7PcS1e",
        "outputId": "28514863-f35b-42cc-bd41-4afd2756a36e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://guereak.com/nst/base_image_1.jpg\n",
            "\u001b[1m2128819/2128819\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1us/step\n",
            "Downloading data from http://guereak.com/nst/style_transfer_3.jpg\n",
            "\u001b[1m5649/5649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "S4ASX5Vvc5G5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set image dimensions\n",
        "original_width, original_height = keras.utils.load_img(base_image_path).size\n",
        "# img_height = 400  # Fixed height\n",
        "# img_width = round(original_width * img_height / original_height)  # Maintain aspect ratio\n",
        "img_height = 32\n",
        "img_width = 32"
      ],
      "metadata": {
        "id": "-1L9C_WIc_Vq"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(image_path):\n",
        "    \"\"\"Loads and preprocesses an image for the model.\"\"\"\n",
        "    img = keras.utils.load_img(image_path, target_size=(img_height, img_width))\n",
        "    img = keras.utils.img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
        "    img = keras.applications.vgg19.preprocess_input(img)\n",
        "    return img\n",
        "\n",
        "def deprocess_image(img):\n",
        "    \"\"\"Converts processed image back to a displayable format.\"\"\"\n",
        "    img = img.reshape((img_height, img_width, 3))\n",
        "    img[:, :, 0] += 103.939  # Undo normalization\n",
        "    img[:, :, 1] += 116.779\n",
        "    img[:, :, 2] += 123.68\n",
        "    img = img[:, :, ::-1]  # Convert from BGR to RGB\n",
        "    img = np.clip(img, 0, 255).astype(\"uint8\")\n",
        "    return img"
      ],
      "metadata": {
        "id": "5herERXtdDbr"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Try with our OWN (small) CNN: based on ResNet-18.\n",
        "\n"
      ],
      "metadata": {
        "id": "sf-Cr7sydEQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install datasets"
      ],
      "metadata": {
        "id": "EQU_fG-h69qI",
        "outputId": "a5c400e5-28d6-4e2f-e10e-ff2241c83081",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/491.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/183.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/143.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/194.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "\n",
        "# Ensure GPU is detected\n",
        "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n",
        "\n",
        "# Load CIFAR10 Dataset from Hugging Face\n",
        "dataset = load_dataset(\"uoft-cs/cifar10\")\n",
        "\n",
        "# Get class names\n",
        "num_classes = 10\n",
        "\n",
        "# Preprocess images\n",
        "def preprocess_data(example):\n",
        "    \"\"\"Preprocesses a dataset example: resize & normalize.\"\"\"\n",
        "    image = example[\"img\"].resize((32, 32)).convert(\"RGB\")\n",
        "    image = np.array(image) / 255.0  # Normalize\n",
        "    label = to_categorical(example[\"label\"], num_classes)\n",
        "    return {\"image\": image, \"label\": label}\n",
        "\n",
        "# Create tf.data pipeline\n",
        "def prepare_dataset(split, batch_size=128, shuffle=True):\n",
        "    ds = dataset[split].map(preprocess_data)\n",
        "    ds = ds.to_tf_dataset(\n",
        "        columns=[\"image\"], label_cols=[\"label\"], batch_size=batch_size, shuffle=shuffle\n",
        "    )\n",
        "    return ds\n",
        "\n",
        "# Create train & validation datasets\n",
        "batch_size = 128\n",
        "train_dataset = prepare_dataset(\"train\", batch_size=batch_size)\n",
        "val_dataset = prepare_dataset(\"test\", batch_size=batch_size, shuffle=False)\n",
        "\n",
        "def FastTinyCNN(input_shape=(32, 32, 3), num_classes=10):\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "    x = tf.keras.layers.SeparableConv2D(32, (3, 3), padding='same', activation='relu')(inputs)\n",
        "    x = tf.keras.layers.MaxPooling2D()(x)\n",
        "\n",
        "    x = tf.keras.layers.SeparableConv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
        "    x = tf.keras.layers.MaxPooling2D()(x)\n",
        "\n",
        "    x = tf.keras.layers.SeparableConv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    return tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Load Model\n",
        "model = FastTinyCNN(num_classes=num_classes)\n",
        "model.summary()\n",
        "\n",
        "# Compile Model\n",
        "optimizer = AdamW(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Callbacks\n",
        "checkpoint = ModelCheckpoint(\"best_model.h5\", monitor=\"val_accuracy\", save_best_only=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, verbose=1)\n",
        "early_stop = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train Model\n",
        "model.fit(\n",
        "    train_dataset,\n",
        "    epochs=5,  # Adjust based on available time\n",
        "    validation_data=val_dataset,\n",
        "    callbacks=[checkpoint, reduce_lr, early_stop]\n",
        ")\n",
        "\n",
        "# Save Final Model\n",
        "model.save(\"fast_tiny_cnn_cifar10.h5\")\n"
      ],
      "metadata": {
        "id": "zlcP1plC66-r",
        "outputId": "270f16e8-96e1-4681-dc64-42c7b07a085c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available: 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_3 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m3\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ separable_conv2d_3 (\u001b[38;5;33mSeparableConv2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m155\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ separable_conv2d_4 (\u001b[38;5;33mSeparableConv2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │           \u001b[38;5;34m2,400\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ separable_conv2d_5 (\u001b[38;5;33mSeparableConv2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │           \u001b[38;5;34m8,896\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_average_pooling2d_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │           \u001b[38;5;34m1,290\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ separable_conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SeparableConv2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">155</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ separable_conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SeparableConv2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,400</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ separable_conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SeparableConv2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,896</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_average_pooling2d_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m12,741\u001b[0m (49.77 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,741</span> (49.77 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m12,741\u001b[0m (49.77 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,741</span> (49.77 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 955ms/step - accuracy: 0.1555 - loss: 2.2063"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m452s\u001b[0m 1s/step - accuracy: 0.1557 - loss: 2.2060 - val_accuracy: 0.2772 - val_loss: 1.9831 - learning_rate: 0.0010\n",
            "Epoch 2/5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 957ms/step - accuracy: 0.2849 - loss: 1.9247"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m448s\u001b[0m 1s/step - accuracy: 0.2849 - loss: 1.9246 - val_accuracy: 0.3284 - val_loss: 1.8259 - learning_rate: 0.0010\n",
            "Epoch 3/5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 959ms/step - accuracy: 0.3326 - loss: 1.8140"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m448s\u001b[0m 1s/step - accuracy: 0.3326 - loss: 1.8139 - val_accuracy: 0.3644 - val_loss: 1.7710 - learning_rate: 0.0010\n",
            "Epoch 4/5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 959ms/step - accuracy: 0.3588 - loss: 1.7548"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m448s\u001b[0m 1s/step - accuracy: 0.3588 - loss: 1.7548 - val_accuracy: 0.3881 - val_loss: 1.6885 - learning_rate: 0.0010\n",
            "Epoch 5/5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 959ms/step - accuracy: 0.3867 - loss: 1.6946"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m448s\u001b[0m 1s/step - accuracy: 0.3867 - loss: 1.6945 - val_accuracy: 0.4144 - val_loss: 1.6106 - learning_rate: 0.0010\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "aVcke6ApLw7u",
        "outputId": "e82ca96d-9c6b-40e9-9f1b-385dcc169353",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['img', 'label'],\n",
              "        num_rows: 50000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['img', 'label'],\n",
              "        num_rows: 10000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using pretrained VGG19"
      ],
      "metadata": {
        "id": "r4CQ_wBzdI9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained VGG19 model (without fully connected layers)\n",
        "model = keras.applications.vgg19.VGG19(weights=\"imagenet\", include_top=False)"
      ],
      "metadata": {
        "id": "79yDEyufdUxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#outputs_dict = {layer.name: layer.output for layer in model.layers}\n",
        "feature_extractor = keras.Model(inputs=model.inputs, outputs=outputs_dict)"
      ],
      "metadata": {
        "id": "VSnHn5MedW-X"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No clue what these do:"
      ],
      "metadata": {
        "id": "dUkSkrpodbUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def content_loss(base_img, combination_img):\n",
        "    \"\"\"Computes content loss (difference between base and generated image).\"\"\"\n",
        "    return tf.reduce_sum(tf.square(combination_img - base_img))\n",
        "\n",
        "def gram_matrix(x):\n",
        "    \"\"\"Computes Gram matrix for style representation.\"\"\"\n",
        "    x = tf.transpose(x, (2, 0, 1))  # Rearrange dimensions\n",
        "    features = tf.reshape(x, (tf.shape(x)[0], -1))  # Flatten\n",
        "    return tf.matmul(features, tf.transpose(features))\n",
        "\n",
        "def style_loss(style_img, combination_img):\n",
        "    \"\"\"Computes style loss using Gram matrices.\"\"\"\n",
        "    S = gram_matrix(style_img)\n",
        "    C = gram_matrix(combination_img)\n",
        "    channels = 3\n",
        "    size = img_height * img_width\n",
        "    return tf.reduce_sum(tf.square(S - C)) / (4.0 * (channels ** 2) * (size ** 2))\n",
        "\n",
        "def total_variation_loss(x):\n",
        "    \"\"\"Computes total variation loss for smoothness.\"\"\"\n",
        "    a = tf.square(x[:, :-1, :-1, :] - x[:, 1:, :-1, :])\n",
        "    b = tf.square(x[:, :-1, :-1, :] - x[:, :-1, 1:, :])\n",
        "    return tf.reduce_sum(tf.pow(a + b, 1.25))"
      ],
      "metadata": {
        "id": "5KdvDUvgddWN"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define layers for style and content extraction\n",
        "style_layer_names = [\n",
        "    \"separable_conv2d_3\", \"separable_conv2d_4\"\n",
        "]\n",
        "content_layer_name = \"separable_conv2d_5\"\n",
        "\n",
        "# Define weights for the losses\n",
        "total_variation_weight = 1e-6\n",
        "style_weight = 1e-6\n",
        "content_weight = 2.5e-8"
      ],
      "metadata": {
        "id": "_O9vRQ-Bdgza"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(combination_image, base_image, style_reference_image):\n",
        "    \"\"\"Computes the total loss combining content, style, and variation losses.\"\"\"\n",
        "    input_tensor = tf.concat([base_image, style_reference_image, combination_image], axis=0)\n",
        "    features = feature_extractor(input_tensor)\n",
        "    loss = tf.zeros(shape=())\n",
        "\n",
        "    # Content loss\n",
        "    layer_features = features[content_layer_name]\n",
        "    base_image_features = layer_features[0, :, :, :]\n",
        "    combination_features = layer_features[2, :, :, :]\n",
        "    loss += content_weight * content_loss(base_image_features, combination_features)\n",
        "\n",
        "    # Style loss\n",
        "    for layer_name in style_layer_names:\n",
        "        layer_features = features[layer_name]\n",
        "        style_reference_features = layer_features[1, :, :, :]\n",
        "        combination_features = layer_features[2, :, :, :]\n",
        "        loss += (style_weight / len(style_layer_names)) * style_loss(style_reference_features, combination_features)\n",
        "\n",
        "    # Total variation loss\n",
        "    loss += total_variation_weight * total_variation_loss(combination_image)\n",
        "\n",
        "    return loss\n",
        "\n",
        "@tf.function\n",
        "def compute_loss_and_grads(combination_image, base_image, style_reference_image):\n",
        "    \"\"\"Computes gradients for the optimization step.\"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss = compute_loss(combination_image, base_image, style_reference_image)\n",
        "    grads = tape.gradient(loss, combination_image)\n",
        "    return loss, grads"
      ],
      "metadata": {
        "id": "LqIca2AadiaK"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load an optimizer: try various approaches"
      ],
      "metadata": {
        "id": "YFYJJ9qZdq0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO try other optimizers\n",
        "optimizer = keras.optimizers.SGD(\n",
        "    keras.optimizers.schedules.ExponentialDecay(\n",
        "        initial_learning_rate=100.0, decay_steps=100, decay_rate=0.96\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "7LXZ2u6MdvL7"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Time to run our code!"
      ],
      "metadata": {
        "id": "gSqJkBSbd5Bp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess images\n",
        "base_image = preprocess_image(base_image_path)\n",
        "style_reference_image = preprocess_image(style_reference_image_path)\n",
        "combination_image = tf.Variable(preprocess_image(base_image_path))"
      ],
      "metadata": {
        "id": "GNM4dAIzd347"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run optimization loop\n",
        "iterations = 4000\n",
        "for i in range(1, iterations + 1):\n",
        "    loss, grads = compute_loss_and_grads(combination_image, base_image, style_reference_image)\n",
        "    optimizer.apply_gradients([(grads, combination_image)])\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        print(f\"Iteration {i}: loss={loss:.2f}\")\n",
        "        img = deprocess_image(combination_image.numpy())\n",
        "        fname = f\"combination_image_at_iteration_{i}.png\"\n",
        "        keras.utils.save_img(fname, img)"
      ],
      "metadata": {
        "id": "K5s0mHgnd-3n",
        "outputId": "3767dfef-71b3-4645-f2d8-b2d718fd7b06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 100: loss=52.67\n",
            "Iteration 200: loss=28.60\n",
            "Iteration 300: loss=21.08\n",
            "Iteration 400: loss=17.54\n",
            "Iteration 500: loss=15.53\n",
            "Iteration 600: loss=14.25\n",
            "Iteration 700: loss=13.35\n",
            "Iteration 800: loss=12.68\n",
            "Iteration 900: loss=12.18\n",
            "Iteration 1000: loss=11.77\n",
            "Iteration 1100: loss=11.44\n",
            "Iteration 1200: loss=11.16\n",
            "Iteration 1300: loss=10.91\n",
            "Iteration 1400: loss=10.71\n",
            "Iteration 1500: loss=10.53\n",
            "Iteration 1600: loss=10.37\n",
            "Iteration 1700: loss=10.22\n",
            "Iteration 1800: loss=10.10\n",
            "Iteration 1900: loss=9.99\n",
            "Iteration 2000: loss=9.89\n",
            "Iteration 2100: loss=9.80\n",
            "Iteration 2200: loss=9.72\n",
            "Iteration 2300: loss=9.65\n",
            "Iteration 2400: loss=9.58\n",
            "Iteration 2500: loss=9.52\n",
            "Iteration 2600: loss=9.46\n",
            "Iteration 2700: loss=9.41\n",
            "Iteration 2800: loss=9.36\n",
            "Iteration 2900: loss=9.31\n",
            "Iteration 3000: loss=9.27\n",
            "Iteration 3100: loss=9.23\n",
            "Iteration 3200: loss=9.20\n",
            "Iteration 3300: loss=9.16\n",
            "Iteration 3400: loss=9.13\n",
            "Iteration 3500: loss=9.10\n",
            "Iteration 3600: loss=9.08\n",
            "Iteration 3700: loss=9.05\n",
            "Iteration 3800: loss=9.03\n",
            "Iteration 3900: loss=9.00\n",
            "Iteration 4000: loss=8.98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "tu2sBP2-OmbS",
        "outputId": "88c3b803-411a-441f-eed3-d1a6b8454c57"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Display final output\n",
        "final_img_path = \"combination_image_at_iteration_4000.png\"\n",
        "plt.imshow(keras.utils.load_img(final_img_path))\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    }
  ]
}